{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rahulsmahajan/starter-bert-wiki-context-0-528?scriptVersionId=143263051\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom string import Template # For generating prompt template\n\nimport os\nimport gc # grabage collector\n# we need to install the sentence transformer and use its embedding to read the faiss index\n#cp stands for a copy. This command is used to copy files or groups of files or directories. \n# The -r option tells rm to remove directories recursively, and the -f option tells it to force the removal of files and directories that are read-only or do not exist\n\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n\n#installing faiss package for reading faiss wikipedia index\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# as per wikipedia faiss index https://www.kaggle.com/datasets/jjinho/wikipedia-2023-07-faiss-index\nimport faiss\nfrom faiss import write_index, read_index\n\n\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\n# installing langchain package# We will use langchain recursive splitter\n!pip install langchain --no-index --find-links=file:///kaggle/input/llm-pkg/\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\n\nfrom tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:32:53.094707Z","iopub.execute_input":"2023-09-16T16:32:53.095296Z","iopub.status.idle":"2023-09-16T16:34:27.280989Z","shell.execute_reply.started":"2023-09-16T16:32:53.095251Z","shell.execute_reply":"2023-09-16T16:34:27.276069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n# Reading the csv file\n#df_train = pd.read_csv(\"./train.csv\")\ndf_train = pd.read_csv(\"/kaggle/input/additional-train-data-for-llm-science-exam/6000_train_examples.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n#df_extra = pd.read_csv('/kaggle/input/additional-train-data-for-llm-science-exam/extra_train_set.csv')\n#df_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T03:57:02.599183Z","iopub.execute_input":"2023-09-17T03:57:02.599612Z","iopub.status.idle":"2023-09-17T03:57:02.704869Z","shell.execute_reply.started":"2023-09-17T03:57:02.599584Z","shell.execute_reply":"2023-09-17T03:57:02.703964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.reset_index(inplace=True)\ndf_train.rename(columns={'index':'id'},inplace=True)\ndf_train = df_train[:2000]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-17T04:17:51.461899Z","iopub.execute_input":"2023-09-17T04:17:51.462392Z","iopub.status.idle":"2023-09-17T04:17:51.476639Z","shell.execute_reply.started":"2023-09-17T04:17:51.462355Z","shell.execute_reply":"2023-09-17T04:17:51.475454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## READING WIKIPEDIA FILES TO FIND CONTEXT","metadata":{}},{"cell_type":"code","source":"# PART 1 - Searching Wikipedia Titles","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:41:04.314844Z","iopub.execute_input":"2023-09-16T16:41:04.315697Z","iopub.status.idle":"2023-09-16T16:41:04.321243Z","shell.execute_reply.started":"2023-09-16T16:41:04.315653Z","shell.execute_reply":"2023-09-16T16:41:04.320165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loadding the wikipedia faiss index. This will be used for searching\nsentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:41:05.330911Z","iopub.execute_input":"2023-09-16T16:41:05.331328Z","iopub.status.idle":"2023-09-16T16:42:40.163996Z","shell.execute_reply.started":"2023-09-16T16:41:05.331276Z","shell.execute_reply":"2023-09-16T16:42:40.162715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating index of prompts i.e q to search for relavnt wikipedia documents\nfrom sentence_transformers import SentenceTransformer\nSIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 32\n\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\nmodel = model.half() # The model.half() method in PyTorch is used to convert a model to half-precision. This can be useful for reducing the memory footprint of a model, as half-precision numbers use half the memory as single-precision numbers","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:42:40.168151Z","iopub.execute_input":"2023-09-16T16:42:40.168962Z","iopub.status.idle":"2023-09-16T16:42:43.210863Z","shell.execute_reply.started":"2023-09-16T16:42:40.168927Z","shell.execute_reply":"2023-09-16T16:42:43.209675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings_train = model.encode(df_train['prompt'].values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings_train = prompt_embeddings_train.detach().cpu().numpy() # detach to remove gradients.\nsearch_score_train, search_index_train = sentence_index.search(prompt_embeddings_train, 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:42:43.213044Z","iopub.execute_input":"2023-09-16T16:42:43.213524Z","iopub.status.idle":"2023-09-16T16:47:35.134701Z","shell.execute_reply.started":"2023-09-16T16:42:43.213487Z","shell.execute_reply":"2023-09-16T16:47:35.133649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del prompt_embeddings_train\n_ = gc.collect() # garbage collector..frees up memmory\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:47:35.137928Z","iopub.execute_input":"2023-09-16T16:47:35.138599Z","iopub.status.idle":"2023-09-16T16:47:35.617065Z","shell.execute_reply.started":"2023-09-16T16:47:35.138559Z","shell.execute_reply":"2023-09-16T16:47:35.616057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt_embeddings_test = model.encode(df_test['prompt'].values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings_test = prompt_embeddings_test.detach().cpu().numpy() # detach to remove gradients.\nsearch_score_test, search_index_test = sentence_index.search(prompt_embeddings_test, 5)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:47:35.618654Z","iopub.execute_input":"2023-09-16T16:47:35.618999Z","iopub.status.idle":"2023-09-16T16:47:49.477701Z","shell.execute_reply.started":"2023-09-16T16:47:35.618968Z","shell.execute_reply":"2023-09-16T16:47:49.476703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndel sentence_index # deleting as not required. otherwise it will give memory issue\n\ndel prompt_embeddings_test\n_ = gc.collect() # garbage collector..frees up memmory\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:47:49.479158Z","iopub.execute_input":"2023-09-16T16:47:49.479732Z","iopub.status.idle":"2023-09-16T16:47:50.542712Z","shell.execute_reply.started":"2023-09-16T16:47:49.479695Z","shell.execute_reply":"2023-09-16T16:47:50.54172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PART 2 - Fetching relavant text of wikipedia documents","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:47:50.544344Z","iopub.execute_input":"2023-09-16T16:47:50.544739Z","iopub.status.idle":"2023-09-16T16:47:50.549743Z","shell.execute_reply.started":"2023-09-16T16:47:50.544705Z","shell.execute_reply":"2023-09-16T16:47:50.548665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n# getting wikipedia documents \ndef wiki_context(search_score,search_index):\n    df_wiki = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                         columns=['id', 'file'])\n    wikipedia_file_data = []\n\n    for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n        scr_idx = idx\n        _df = df_wiki.loc[scr_idx].copy()\n        _df['prompt_id'] = i\n        wikipedia_file_data.append(_df)\n    wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n    wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n    WIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\n    wiki_files = os.listdir(WIKI_PATH)\n\n    wiki_text_data = []\n\n    for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n        _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n        _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n\n        _df_temp = _df[_df['id'].isin(_id)].copy()\n        del _df\n        _ = gc.collect()\n        libc.malloc_trim(0)\n        wiki_text_data.append(_df_temp)\n    wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n    del df_wiki\n    _ = gc.collect()\n    libc.malloc_trim(0)\n    context_df = wikipedia_file_data.merge(wiki_text_data,on='id')\n    return context_df","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:47:50.551429Z","iopub.execute_input":"2023-09-16T16:47:50.552691Z","iopub.status.idle":"2023-09-16T16:47:50.566836Z","shell.execute_reply.started":"2023-09-16T16:47:50.552655Z","shell.execute_reply":"2023-09-16T16:47:50.56587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del df_wiki\n# _ = gc.collect()\n# libc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:47:50.568421Z","iopub.execute_input":"2023-09-16T16:47:50.568785Z","iopub.status.idle":"2023-09-16T16:47:50.581083Z","shell.execute_reply.started":"2023-09-16T16:47:50.568753Z","shell.execute_reply":"2023-09-16T16:47:50.579877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_df_train = wiki_context(search_score_train,search_index_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:47:50.586733Z","iopub.execute_input":"2023-09-16T16:47:50.587053Z","iopub.status.idle":"2023-09-16T16:52:44.958766Z","shell.execute_reply.started":"2023-09-16T16:47:50.587028Z","shell.execute_reply":"2023-09-16T16:52:44.957725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context_df_test = wiki_context(search_score_test,search_index_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:52:44.96114Z","iopub.execute_input":"2023-09-16T16:52:44.961855Z","iopub.status.idle":"2023-09-16T16:57:30.751535Z","shell.execute_reply.started":"2023-09-16T16:52:44.961819Z","shell.execute_reply":"2023-09-16T16:57:30.750458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Spliting the wiki text in the context df in chunk size\n\nchunk_size = 1200\nchunk_overlap = 200\n\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n\nsplit_text =[]\nfor i in range(len(context_df_train)):\n    split_text.append ( r_splitter.split_text(context_df_train.loc[i,'text']))\ncontext_df_train['split'] = split_text\n","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:57:30.753168Z","iopub.execute_input":"2023-09-16T16:57:30.753768Z","iopub.status.idle":"2023-09-16T16:58:38.508789Z","shell.execute_reply.started":"2023-09-16T16:57:30.75373Z","shell.execute_reply":"2023-09-16T16:58:38.507676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_dataframe(df, context_df):\n    \n    model = SentenceTransformer(SIM_MODEL, device='cuda')\n    model.max_seq_length = MAX_LENGTH\n    model = model.half() # The model.half() method in PyTorch is used to convert a model to half-precision. This can be useful for reducing the memory footprint of a model, as half-precision numbers use half the memory as single-precision numbers\n    \n #   final_prompt = []\n    #for i in range(5):\n    for i in range(len(df)):\n        q = df.iloc[i]['prompt']\n        idx = df.iloc[i]['id']\n        chunk = ''\n        \n        text_rel = context_df[context_df['prompt_id'] == idx].iloc[:]['split']\n        text = []\n        for j in range(len(text_rel)):\n            text.extend(text_rel.iloc[j])\n        if text != []:\n            text_df = pd.DataFrame(text,columns=['text'])\n            vectors = model.encode(text_df['text'])\n            vector_dimension = vectors.shape[1]\n            index = faiss.IndexFlatL2(vector_dimension)\n            faiss.normalize_L2(vectors)\n            index.add(vectors)\n\n\n            search_vector = model.encode(q)\n            _vector = np.array([search_vector])\n            faiss.normalize_L2(_vector)\n\n            k = 1\n            distances, ann = index.search(_vector, k=k)\n            chunk = text[ann[0,0]]\n\n        df.iloc[i,1] = str(chunk) + ' ### ' + q\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-09-16T16:58:38.51026Z","iopub.execute_input":"2023-09-16T16:58:38.510685Z","iopub.status.idle":"2023-09-16T16:58:38.523856Z","shell.execute_reply.started":"2023-09-16T16:58:38.510646Z","shell.execute_reply":"2023-09-16T16:58:38.521796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_train_df = format_dataframe(df_train,context_df_train)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T17:01:41.269862Z","iopub.execute_input":"2023-09-16T17:01:41.270787Z","iopub.status.idle":"2023-09-16T17:15:54.241083Z","shell.execute_reply.started":"2023-09-16T17:01:41.270741Z","shell.execute_reply":"2023-09-16T17:15:54.240145Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## TRAINING THE MODEL","metadata":{}},{"cell_type":"code","source":"#model_train[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\",\"answer\"]].to_csv(\"./train_context.csv\", index=False)\n#model_train_df = pd.read_csv(\"train_context.csv\")\n#model_train_df.index = list(range(len(model_train_df)))\n#model_train_df['id'] = list(range(len(model_train_df)))\n#model_train_df['context'] = model_train_df['context'].apply(lambda x: str(x))\n#model_train_df[\"prompt\"] = model_train_df[\"context\"] + \" #### \" +  model_train_df[\"prompt\"]\n#model_train_df['answer'] = 'B'","metadata":{"execution":{"iopub.status.busy":"2023-09-16T17:15:54.243399Z","iopub.execute_input":"2023-09-16T17:15:54.243853Z","iopub.status.idle":"2023-09-16T17:15:54.248847Z","shell.execute_reply.started":"2023-09-16T17:15:54.243819Z","shell.execute_reply":"2023-09-16T17:15:54.247765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoModelForMultipleChoice,Trainer,TrainingArguments\n# For convenience we'll turn our pandas Dataframe into a Dataset\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\n# The path of the model checkpoint we want to use\nmodel_dir = '/kaggle/input/huggingface-bert/bert-base-cased'\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\n#tokenizer(truncation=True,max_length=512,text_target='None')\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-16T17:15:54.250119Z","iopub.execute_input":"2023-09-16T17:15:54.250632Z","iopub.status.idle":"2023-09-16T17:15:59.747806Z","shell.execute_reply.started":"2023-09-16T17:15:54.250599Z","shell.execute_reply":"2023-09-16T17:15:59.746872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_examples = len(model_train_df)\nsplit_size = 0.65*total_examples\nsplit_size = int(split_size)\ntrain_ds = Dataset.from_pandas(model_train_df[:split_size])\neval_ds = Dataset.from_pandas(model_train_df[split_size:])","metadata":{"execution":{"iopub.status.busy":"2023-09-16T17:16:03.550592Z","iopub.execute_input":"2023-09-16T17:16:03.55096Z","iopub.status.idle":"2023-09-16T17:16:03.596437Z","shell.execute_reply.started":"2023-09-16T17:16:03.550931Z","shell.execute_reply":"2023-09-16T17:16:03.595216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\noptions = 'ABCDE'\nindices = list(range(5))\n\noption_to_index = {option: index for option, index in zip(options, indices)}\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\ndef preprocess(example):\n    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n    # so we'll copy our question 5 times before tokenizing\n    first_sentence = [example['prompt']] * 5\n    second_sentence = []\n    for option in options:\n        second_sentence.append(str(example[option]))\n    # Our tokenizer will turn our text into token IDs BERT can understand\n    tokenized_example = tokenizer(first_sentence, second_sentence,padding=True, truncation=True,max_length=256)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    return tokenized_example\n","metadata":{"execution":{"iopub.status.busy":"2023-09-16T17:18:52.615467Z","iopub.execute_input":"2023-09-16T17:18:52.615921Z","iopub.status.idle":"2023-09-16T17:18:52.624035Z","shell.execute_reply.started":"2023-09-16T17:18:52.615889Z","shell.execute_reply":"2023-09-16T17:18:52.623107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_train_ds = train_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_eval_ds = eval_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])","metadata":{"execution":{"iopub.status.busy":"2023-09-16T17:19:22.051941Z","iopub.execute_input":"2023-09-16T17:19:22.052333Z","iopub.status.idle":"2023-09-16T17:19:54.682651Z","shell.execute_reply.started":"2023-09-16T17:19:22.052285Z","shell.execute_reply":"2023-09-16T17:19:54.681698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Following datacollator (adapted from https://huggingface.co/docs/transformers/tasks/multiple_choice)\n# will dynamically pad our questions at batch-time so we don't have to make every question the length\n# of our longest question.\nfrom datasets import Dataset\nfrom dataclasses import dataclass\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom typing import Optional, Union\nimport torch\n\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = 'max_length'\n    max_length: Optional[int] = 512\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch\n","metadata":{"execution":{"iopub.status.busy":"2023-09-16T17:19:59.369979Z","iopub.execute_input":"2023-09-16T17:19:59.370383Z","iopub.status.idle":"2023-09-16T17:19:59.381595Z","shell.execute_reply.started":"2023-09-16T17:19:59.370348Z","shell.execute_reply":"2023-09-16T17:19:59.380614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The arguments here are selected to run quickly; feel free to play with them.\nimport shutil\n#shutil.rmtree('finetuned_bert')\nmodel_dir = 'finetuned_bert'\ntraining_args = TrainingArguments(\n    output_dir=model_dir,\n    evaluation_strategy=\"epoch\",\n#    save_strategy=\"epoch\",\n#    load_best_model_at_end=True,\n    learning_rate=5e-5,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    report_to='none'\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T17:20:11.323169Z","iopub.execute_input":"2023-09-16T17:20:11.324166Z","iopub.status.idle":"2023-09-16T17:20:11.332586Z","shell.execute_reply.started":"2023-09-16T17:20:11.324129Z","shell.execute_reply":"2023-09-16T17:20:11.33156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generally it's a bad idea to validate on your training set, but because our training set\n# for this problem is so small we're going to train on all our data.\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_ds,\n    eval_dataset=tokenized_eval_ds,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T17:20:12.963241Z","iopub.execute_input":"2023-09-16T17:20:12.963664Z","iopub.status.idle":"2023-09-16T17:20:12.984737Z","shell.execute_reply.started":"2023-09-16T17:20:12.963633Z","shell.execute_reply":"2023-09-16T17:20:12.983643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n_ = gc.collect() # garbage collector..frees up memmory\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T17:20:15.559464Z","iopub.execute_input":"2023-09-16T17:20:15.559838Z","iopub.status.idle":"2023-09-16T17:20:17.404347Z","shell.execute_reply.started":"2023-09-16T17:20:15.559809Z","shell.execute_reply":"2023-09-16T17:20:17.403301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training should take about a minute\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-09-16T17:20:18.934775Z","iopub.execute_input":"2023-09-16T17:20:18.935762Z","iopub.status.idle":"2023-09-16T18:03:09.062503Z","shell.execute_reply.started":"2023-09-16T17:20:18.935725Z","shell.execute_reply":"2023-09-16T18:03:09.061254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The following function gets the indices of the highest scoring answers for each row\n# and converts them back to our answer format (A, B, C, D, E)\nimport numpy as np\ndef predictions_to_map_output(predictions):\n    sorted_answer_indices = np.argsort(-predictions)\n    top_answer_indices = sorted_answer_indices[:,:3] # Get the first three answers in each row\n    top_answers = np.vectorize(index_to_option.get)(top_answer_indices)\n    return np.apply_along_axis(lambda row: ' '.join(row), 1, top_answers)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:52:58.995461Z","iopub.execute_input":"2023-09-16T14:52:58.996239Z","iopub.status.idle":"2023-09-16T14:52:59.003139Z","shell.execute_reply.started":"2023-09-16T14:52:58.996189Z","shell.execute_reply":"2023-09-16T14:52:59.00222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## INFERENCING ON TEST DF","metadata":{}},{"cell_type":"code","source":"del df_train\ndel context_df_train\ndel model_train_df\ndel tokenized_train_ds\ndel tokenized_eval_ds\n\n_ = gc.collect() # garbage collector..frees up memmory\nlibc.malloc_trim(0)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:53:36.191164Z","iopub.execute_input":"2023-09-16T14:53:36.191544Z","iopub.status.idle":"2023-09-16T14:53:36.581169Z","shell.execute_reply.started":"2023-09-16T14:53:36.191511Z","shell.execute_reply":"2023-09-16T14:53:36.580109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunk_size = 1200\nchunk_overlap = 200\n\nr_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,\n    chunk_overlap=chunk_overlap\n)\n\nsplit_text =[]\nfor i in range(len(context_df_test)):\n    split_text.append ( r_splitter.split_text(context_df_test.loc[i,'text']))\ncontext_df_test['split'] = split_text","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:54:08.652885Z","iopub.execute_input":"2023-09-16T14:54:08.65328Z","iopub.status.idle":"2023-09-16T14:54:12.746508Z","shell.execute_reply.started":"2023-09-16T14:54:08.653249Z","shell.execute_reply":"2023-09-16T14:54:12.745502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = format_dataframe(df_test,context_df_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:54:14.532477Z","iopub.execute_input":"2023-09-16T14:54:14.532847Z","iopub.status.idle":"2023-09-16T14:54:56.816062Z","shell.execute_reply.started":"2023-09-16T14:54:14.532814Z","shell.execute_reply":"2023-09-16T14:54:56.815148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model_test[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)\n#test_df = pd.read_csv(\"test_context.csv\")\n#test_df.index = list(range(len(test_df)))\n#test_df['id'] = list(range(len(test_df)))\n#test_df['context'] = test_df['context'].apply(lambda x: str(x))\n#test_df[\"prompt\"] = test_df[\"context\"] + \" #### \" +  test_df[\"prompt\"]\ntest_df['answer'] = 'B'","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:55:01.874966Z","iopub.execute_input":"2023-09-16T14:55:01.875367Z","iopub.status.idle":"2023-09-16T14:55:01.880775Z","shell.execute_reply.started":"2023-09-16T14:55:01.875336Z","shell.execute_reply":"2023-09-16T14:55:01.879839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = Dataset.from_pandas(test_df)\ntokenized_test_ds = test_ds.map(preprocess, batched=False, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:55:03.507447Z","iopub.execute_input":"2023-09-16T14:55:03.508148Z","iopub.status.idle":"2023-09-16T14:55:04.522544Z","shell.execute_reply.started":"2023-09-16T14:55:03.508112Z","shell.execute_reply":"2023-09-16T14:55:04.521599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we'll generate our \"real\" predictions on the test set\ntest_predictions = trainer.predict(tokenized_test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:55:05.480375Z","iopub.execute_input":"2023-09-16T14:55:05.480732Z","iopub.status.idle":"2023-09-16T14:55:23.763106Z","shell.execute_reply.started":"2023-09-16T14:55:05.480697Z","shell.execute_reply":"2023-09-16T14:55:23.762197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = test_df[['id']]\nsubmission_df['prediction'] = predictions_to_map_output(test_predictions.predictions)\n\nsubmission_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-16T14:55:24.785598Z","iopub.execute_input":"2023-09-16T14:55:24.786Z","iopub.status.idle":"2023-09-16T14:55:24.816832Z","shell.execute_reply.started":"2023-09-16T14:55:24.785966Z","shell.execute_reply":"2023-09-16T14:55:24.815929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Once we write our submission file we're good to submit!\nif os.path.exists('submission.csv'):\n    os.remove('submission.csv')\nsubmission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-17T04:07:24.651005Z","iopub.execute_input":"2023-09-17T04:07:24.651367Z","iopub.status.idle":"2023-09-17T04:07:24.658475Z","shell.execute_reply.started":"2023-09-17T04:07:24.65134Z","shell.execute_reply":"2023-09-17T04:07:24.657337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}